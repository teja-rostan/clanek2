\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{augasta2013pruning}
\citation{augasta2013pruning}
\citation{DBLP:journals/corr/GongLYB14}
\citation{zeiler2014visualizing}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{bondarenko2014artificial,schmidhuber2015deep,sainath2013low}
\citation{han2015learning}
\citation{lecun1989optimal,hassibi1993optimal}
\citation{DBLP:journals/corr/GongLYB14}
\citation{xue2013restructuring}
\citation{collins2014memory}
\citation{han2015learning}
\citation{weinberger2009feature,shi2009hash,chen2015compressing}
\citation{mathieu2013fast}
\citation{chetlur2014cudnn}
\citation{chetlur2014cudnn}
\citation{chellapilla2006high}
\citation{anwar2015structured}
\citation{jaderberg2014speeding,rigamonti2013learning}
\citation{jaderberg2014speeding}
\citation{denton2014exploiting}
\citation{zhang2015efficient}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Approximation of weights in neural network}{2}{section.3}}
\citation{zitnik2015data}
\citation{zitnik2015data}
\citation{sainath2013low}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Deep neural network with dense connected layers. Relation matrix $W_{i,j}$ stores the weights of connections between neurons at layer $i$ and $j$.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f:globokamreza}{{1}{3}{Deep neural network with dense connected layers. Relation matrix $W_{i,j}$ stores the weights of connections between neurons at layer $i$ and $j$.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Approximation of weights in dense layers}{3}{subsection.3.1}}
\newlabel{t:2}{{3.1}{3}{Approximation of weights in dense layers}{theorem.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graphical visualization of simultaneous matrix tri-factorization.\relax }}{3}{figure.caption.2}}
\newlabel{f:mf2}{{2}{3}{Graphical visualization of simultaneous matrix tri-factorization.\relax }{figure.caption.2}{}}
\newlabel{eq:1}{{1}{3}{Approximation of weights in dense layers}{equation.3.1}{}}
\citation{hoyer2004non}
\citation{Zitnik2012}
\citation{code}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{krizhevsky2009learning}
\newlabel{eq:2}{{2}{4}{Approximation of weights in dense layers}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Approximation of weights in convolutional layers}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Factorization-based brain pruning (MFBP)}{4}{section.4}}
\newlabel{alg:MFBP}{{1}{4}{Factorization-based brain pruning (MFBP)}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pruning neural network with matrix factorization.\relax }}{4}{algocf.1}}
\citation{github}
\citation{AISTATS2011_GlorotBB11}
\citation{srivastava2014dropout}
\citation{lecture}
\citation{Bastien-Theano-2012,bergstra+al:2010-scipy}
\citation{github}
\citation{zitnik2015data}
\citation{Zitnik2012}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental setup}{5}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Results on dense layers}{5}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Results on convolutional layers}{5}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces AUC results of six networks after pruning\relax }}{6}{figure.caption.3}}
\newlabel{f:results}{{3}{6}{AUC results of six networks after pruning\relax }{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces AUC results from before pruning (BP) and after pruning (AP).\relax }}{6}{table.caption.4}}
\newlabel{t:results}{{1}{6}{AUC results from before pruning (BP) and after pruning (AP).\relax }{table.caption.4}{}}
\citation{anwar2015structured}
\bibdata{literature}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces AUC results of two approaches on MNIST and CIFAR dataset.\relax }}{7}{figure.caption.5}}
\newlabel{f:results_conv}{{4}{7}{AUC results of two approaches on MNIST and CIFAR dataset.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Degree of pruning.\relax }}{7}{figure.caption.6}}
\newlabel{f:degree_conv}{{5}{7}{Degree of pruning.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and conclusion}{7}{section.7}}
\bibcite{anwar2015structured}{{1}{}{{}}{{}}}
\bibcite{augasta2013pruning}{{2}{}{{}}{{}}}
\bibcite{Bastien-Theano-2012}{{3}{}{{}}{{}}}
\bibcite{bergstra+al:2010-scipy}{{4}{}{{}}{{}}}
\bibcite{bondarenko2014artificial}{{5}{}{{}}{{}}}
\bibcite{chellapilla2006high}{{6}{}{{}}{{}}}
\bibcite{chen2015compressing}{{7}{}{{}}{{}}}
\bibcite{chetlur2014cudnn}{{8}{}{{}}{{}}}
\bibcite{collins2014memory}{{9}{}{{}}{{}}}
\bibcite{denton2014exploiting}{{10}{}{{}}{{}}}
\bibcite{AISTATS2011_GlorotBB11}{{11}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/GongLYB14}{{12}{}{{}}{{}}}
\bibcite{han2015learning}{{13}{}{{}}{{}}}
\bibcite{hassibi1993optimal}{{14}{}{{}}{{}}}
\bibcite{lecture}{{15}{}{{}}{{}}}
\bibcite{hoyer2004non}{{16}{}{{}}{{}}}
\bibcite{jaderberg2014speeding}{{17}{}{{}}{{}}}
\bibcite{krizhevsky2009learning}{{18}{}{{}}{{}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{19}{}{{}}{{}}}
\bibcite{lecun1989optimal}{{20}{}{{}}{{}}}
\bibcite{mathieu2013fast}{{21}{}{{}}{{}}}
\bibcite{github}{{22}{}{{}}{{}}}
\bibcite{scikit-learn}{{23}{}{{}}{{}}}
\bibcite{rigamonti2013learning}{{24}{}{{}}{{}}}
\bibcite{code}{{25}{}{{}}{{}}}
\bibcite{sainath2013low}{{26}{}{{}}{{}}}
\bibcite{schmidhuber2015deep}{{27}{}{{}}{{}}}
\bibcite{shi2009hash}{{28}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{29}{}{{}}{{}}}
\bibcite{weinberger2009feature}{{30}{}{{}}{{}}}
\bibcite{xue2013restructuring}{{31}{}{{}}{{}}}
\bibcite{zeiler2014visualizing}{{32}{}{{}}{{}}}
\bibcite{zhang2015efficient}{{33}{}{{}}{{}}}
\bibcite{Zitnik2012}{{34}{}{{}}{{}}}
\bibcite{zitnik2015data}{{35}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
