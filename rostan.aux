\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{augasta2013pruning}
\citation{augasta2013pruning}
\citation{DBLP:journals/corr/GongLYB14}
\citation{augasta2013pruning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{zeiler2014visualizing}
\citation{bondarenko2014artificial,schmidhuber2015deep,sainath2013low}
\citation{han2015learning}
\citation{lecun1989optimal}
\citation{hassibi1993optimal}
\citation{DBLP:journals/corr/GongLYB14}
\citation{xue2013restructuring}
\citation{collins2014memory}
\citation{han2015learning}
\citation{weinberger2009feature,shi2009hash,chen2015compressing}
\citation{yang2014deep}
\citation{mathieu2013fast}
\citation{chetlur2014cudnn}
\citation{chetlur2014cudnn}
\citation{chellapilla2006high}
\citation{anwar2015structured}
\citation{jaderberg2014speeding,rigamonti2013learning}
\citation{jaderberg2014speeding}
\citation{denton2014exploiting}
\citation{zhang2015efficient}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}}
\citation{zitnik2015data}
\citation{zitnik2015data}
\@writefile{toc}{\contentsline {section}{\numberline {3}Approximation of network weights with simultaneous matrix tri-factorization}{3}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Deep neural network with dense connected layers. Relation matrix $W_{i,j}$ stores the weights of connections between neurons at layer $i$ and $j$.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f:globokamreza}{{1}{3}{Deep neural network with dense connected layers. Relation matrix $W_{i,j}$ stores the weights of connections between neurons at layer $i$ and $j$.\relax }{figure.caption.1}{}}
\newlabel{t:2}{{3.1}{3}{Approximation of network weights with simultaneous matrix tri-factorization}{theorem.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graphical visualization of simultaneous matrix tri-factorization.\relax }}{3}{figure.caption.2}}
\newlabel{f:mf2}{{2}{3}{Graphical visualization of simultaneous matrix tri-factorization.\relax }{figure.caption.2}{}}
\citation{sainath2013low}
\citation{code}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{github}
\citation{AISTATS2011_GlorotBB11}
\citation{srivastava2014dropout}
\citation{lecture}
\newlabel{eq:1}{{1}{4}{Approximation of network weights with simultaneous matrix tri-factorization}{equation.3.1}{}}
\newlabel{eq:2}{{2}{4}{Approximation of network weights with simultaneous matrix tri-factorization}{equation.3.2}{}}
\newlabel{alg:MFBP}{{1}{4}{Approximation of network weights with simultaneous matrix tri-factorization}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pruning neural network with simultaneous matrix tri-factorization.\relax }}{4}{algocf.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental setup}{4}{section.4}}
\citation{Bastien-Theano-2012,bergstra+al:2010-scipy}
\citation{github}
\citation{zitnik2015data}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{5}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces AUC results of six networks after pruning\relax }}{5}{figure.caption.3}}
\newlabel{f:results}{{3}{5}{AUC results of six networks after pruning\relax }{figure.caption.3}{}}
\bibdata{literature}
\bibcite{anwar2015structured}{{1}{}{{}}{{}}}
\bibcite{augasta2013pruning}{{2}{}{{}}{{}}}
\bibcite{Bastien-Theano-2012}{{3}{}{{}}{{}}}
\bibcite{bergstra+al:2010-scipy}{{4}{}{{}}{{}}}
\bibcite{bondarenko2014artificial}{{5}{}{{}}{{}}}
\bibcite{chellapilla2006high}{{6}{}{{}}{{}}}
\bibcite{chen2015compressing}{{7}{}{{}}{{}}}
\bibcite{chetlur2014cudnn}{{8}{}{{}}{{}}}
\bibcite{collins2014memory}{{9}{}{{}}{{}}}
\bibcite{denton2014exploiting}{{10}{}{{}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces AUC results from before pruning (BP) and after pruning (AP).\relax }}{6}{table.caption.4}}
\newlabel{t:results}{{1}{6}{AUC results from before pruning (BP) and after pruning (AP).\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion and conclusion}{6}{section.6}}
\bibcite{AISTATS2011_GlorotBB11}{{11}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/GongLYB14}{{12}{}{{}}{{}}}
\bibcite{han2015learning}{{13}{}{{}}{{}}}
\bibcite{hassibi1993optimal}{{14}{}{{}}{{}}}
\bibcite{lecture}{{15}{}{{}}{{}}}
\bibcite{jaderberg2014speeding}{{16}{}{{}}{{}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{17}{}{{}}{{}}}
\bibcite{lecun1989optimal}{{18}{}{{}}{{}}}
\bibcite{mathieu2013fast}{{19}{}{{}}{{}}}
\bibcite{github}{{20}{}{{}}{{}}}
\bibcite{scikit-learn}{{21}{}{{}}{{}}}
\bibcite{rigamonti2013learning}{{22}{}{{}}{{}}}
\bibcite{code}{{23}{}{{}}{{}}}
\bibcite{sainath2013low}{{24}{}{{}}{{}}}
\bibcite{schmidhuber2015deep}{{25}{}{{}}{{}}}
\bibcite{shi2009hash}{{26}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{27}{}{{}}{{}}}
\bibcite{weinberger2009feature}{{28}{}{{}}{{}}}
\bibcite{xue2013restructuring}{{29}{}{{}}{{}}}
\bibcite{yang2014deep}{{30}{}{{}}{{}}}
\bibcite{zeiler2014visualizing}{{31}{}{{}}{{}}}
\bibcite{zhang2015efficient}{{32}{}{{}}{{}}}
\bibcite{zitnik2015data}{{33}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
